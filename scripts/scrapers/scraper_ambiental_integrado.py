#!/usr/bin/env python3
"""
Scraper integrado para datos ambientales del SEA
Usa scraper especializado para obtener proyectos del SEA
"""

import logging
import time
from datetime import datetime
from typing import List, Dict, Optional
import requests

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s'
)
logger = logging.getLogger(__name__)

# Importar scrapers directamente sin fallbacks complejos
try:
    # Intentar importaci√≥n relativa primero
    from .scraper_sea_selenium_completo import ScraperSEASeleniumCompleto
    logger.info("‚úÖ Importado ScraperSEASeleniumCompleto (relativo)")
except ImportError:
    try:
        # Intentar importaci√≥n absoluta
        from scripts.scrapers.scraper_sea_selenium_completo import ScraperSEASeleniumCompleto
        logger.info("‚úÖ Importado ScraperSEASeleniumCompleto (absoluto)")
    except ImportError:
        try:
            # Intentar importaci√≥n directa (cuando se ejecuta desde el mismo directorio)
            from scraper_sea_selenium_completo import ScraperSEASeleniumCompleto
            logger.info("‚úÖ Importado ScraperSEASeleniumCompleto (directo)")
        except ImportError as e:
            logger.error(f"‚ùå Error importando ScraperSEASeleniumCompleto: {e}")
            ScraperSEASeleniumCompleto = None


# Importar telemetr√≠a
try:
    from telemetria_ambiental import TelemetriaScrapers
    telemetria = TelemetriaScrapers()
except ImportError:
    logger.warning("‚ö†Ô∏è Telemetr√≠a no disponible")
    telemetria = None

# Importar extractor de res√∫menes SEA
try:
    from .sea_resumen_extractor import sea_resumen_extractor
    logger.info("‚úÖ Importado extractor de res√∫menes SEA")
except ImportError:
    try:
        from scripts.scrapers.sea_resumen_extractor import sea_resumen_extractor
        logger.info("‚úÖ Importado extractor de res√∫menes SEA (absoluto)")
    except ImportError:
        try:
            from sea_resumen_extractor import sea_resumen_extractor
            logger.info("‚úÖ Importado extractor de res√∫menes SEA (directo)")
        except ImportError:
            logger.warning("‚ö†Ô∏è Extractor de res√∫menes SEA no disponible")
            sea_resumen_extractor = None

class ScraperAmbiental:
    def __init__(self):
        """Inicializa el scraper ambiental integrado"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9'
        })
        
        # Inicializar scraper SEA
        if ScraperSEASeleniumCompleto:
            self.scraper_sea = ScraperSEASeleniumCompleto()
            logger.info("‚úÖ Scraper SEA con Selenium inicializado")
        else:
            self.scraper_sea = None
            logger.error("‚ùå Scraper SEA no disponible")
    
    def obtener_datos_ambientales(self, dias_atras: int = 7) -> Dict:
        """
        Obtiene datos ambientales del SEA
        
        Args:
            dias_atras: N√∫mero de d√≠as hacia atr√°s para buscar
            
        Returns:
            Diccionario con proyectos SEA
        """
        logger.info(f"üîç Obteniendo datos ambientales de los √∫ltimos {dias_atras} d√≠as...")
        
        inicio = time.time()
        errores_totales = []
        
        # Obtener proyectos SEA
        proyectos_sea = []
        if self.scraper_sea:
            try:
                logger.info("üìã Obteniendo proyectos SEA...")
                proyectos_sea = self._obtener_proyectos_sea(dias_atras)
                
                if telemetria:
                    telemetria.registrar_extraccion('SEA', {
                        'total_items': len(proyectos_sea),
                        'tiempo_ms': (time.time() - inicio) * 1000,
                        'exitoso': True
                    })
                    
                if proyectos_sea:
                    logger.info(f"‚úÖ {len(proyectos_sea)} proyectos SEA encontrados")
                else:
                    logger.warning("‚ö†Ô∏è No se encontraron proyectos SEA")
                    errores_totales.append("SEA: No se encontraron proyectos")
                    
            except Exception as e:
                logger.error(f"‚ùå Error obteniendo datos SEA: {str(e)}")
                errores_totales.append(f"SEA: {str(e)}")
                
                if telemetria:
                    telemetria.registrar_extraccion('SEA', {
                        'total_items': 0,
                        'tiempo_ms': (time.time() - inicio) * 1000,
                        'exitoso': False,
                        'errores': [str(e)]
                    })
        else:
            logger.warning("‚ö†Ô∏è Scraper SEA no disponible")
            errores_totales.append("SEA: Scraper no disponible")
        
        
        tiempo_total = (time.time() - inicio) * 1000
        
        # Si hay errores, registrarlos
        if errores_totales:
            logger.error(f"‚ùå Errores registrados: {errores_totales}")
        
        datos = {
            'proyectos_sea': proyectos_sea,
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'tiempo_total_ms': tiempo_total,
                'errores': errores_totales,
                'telemetria': {
                    'sea_items': len(proyectos_sea),
                    'total_items': len(proyectos_sea)
                }
            }
        }
        
        logger.info(f"‚úÖ Datos ambientales obtenidos en {tiempo_total:.0f}ms - SEA: {len(proyectos_sea)}")
        
        return datos
    
    def _obtener_proyectos_sea(self, dias_atras: int) -> List[Dict]:
        """
        Obtiene proyectos del SEA usando el scraper con Selenium
        """
        try:
            logger.info(f"üîÑ Obteniendo proyectos SEA de los √∫ltimos {dias_atras} d√≠as...")
            proyectos = self.scraper_sea.obtener_datos_sea(dias_atras=dias_atras)
            
            if proyectos:
                logger.info(f"‚úÖ {len(proyectos)} proyectos SEA obtenidos")
                # Formatear para el informe
                proyectos_formateados = []
                for p in proyectos:
                    proyecto = {
                        'fuente': 'SEA',
                        'fecha_extraccion': datetime.now().isoformat(),
                        'titulo': p.get('titulo', ''),
                        'url': p.get('url', ''),
                        'id': p.get('id_expediente', ''),
                        'tipo': p.get('tipo', ''),
                        'region': p.get('region', ''),
                        'comuna': p.get('comuna', ''),
                        'tipo_proyecto': p.get('tipo_proyecto', ''),
                        'razon_ingreso': p.get('razon_ingreso', ''),
                        'empresa': p.get('empresa', p.get('titular', '')),
                        'inversion': p.get('inversion_mmusd', p.get('inversion', '')),
                        'fecha': p.get('fecha_presentacion', ''),
                        'resumen': p.get('resumen_completo', p.get('resumen', ''))
                    }
                    proyectos_formateados.append(proyecto)
                return proyectos_formateados
            else:
                logger.warning("‚ö†Ô∏è No se encontraron proyectos SEA")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Error en _obtener_proyectos_sea: {str(e)}")
            raise
    
    
    def formatear_para_informe(self, datos_ambientales: Dict) -> Dict:
        """
        Formatea los datos ambientales para el informe HTML
        Solo usa datos REALES obtenidos del scraper SEA
        Incluye res√∫menes ejecutivos de los proyectos
        """
        resultado = {
            'proyectos_sea': []
        }
        
        # Formatear proyectos SEA reales
        if datos_ambientales.get('proyectos_sea'):
            for proyecto in datos_ambientales['proyectos_sea']:
                # Solo incluir si tiene t√≠tulo (es un proyecto real)
                if proyecto.get('titulo'):
                    # Intentar obtener resumen si tenemos URL y el extractor est√° disponible
                    if not proyecto.get('resumen') and proyecto.get('url') and sea_resumen_extractor:
                        try:
                            # Extraer ID del expediente de la URL
                            id_expediente = sea_resumen_extractor.obtener_id_de_url(proyecto['url'])
                            if id_expediente:
                                logger.info(f"üîç Obteniendo resumen para proyecto {proyecto['titulo'][:30]}...")
                                info_extra = sea_resumen_extractor.extraer_resumen_proyecto(id_expediente)
                                
                                # Agregar resumen y otros datos si los encontramos
                                if info_extra.get('resumen'):
                                    proyecto['resumen'] = info_extra['resumen']
                                    logger.info(f"‚úÖ Resumen obtenido ({len(info_extra['resumen'])} caracteres)")
                                
                                # Agregar inversi√≥n si la encontramos
                                if info_extra.get('inversion') and not proyecto.get('inversion'):
                                    proyecto['inversion'] = info_extra['inversion']
                                
                                # Agregar ubicaci√≥n m√°s detallada si la encontramos
                                if info_extra.get('ubicacion') and not proyecto.get('region'):
                                    proyecto['region'] = info_extra['ubicacion']
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è No se pudo obtener resumen para {proyecto['titulo'][:30]}: {e}")
                    
                    # Si no hay resumen, crear uno b√°sico con la informaci√≥n disponible
                    if not proyecto.get('resumen'):
                        resumen_basico = f"{proyecto.get('tipo', 'DIA')} presentado"
                        if proyecto.get('titular'):
                            resumen_basico += f" por {proyecto['titular']}"
                        if proyecto.get('region'):
                            resumen_basico += f" en {proyecto['region']}"
                        if proyecto.get('inversion'):
                            resumen_basico += f". Inversi√≥n: {proyecto['inversion']}"
                        proyecto['resumen'] = resumen_basico
                    
                    resultado['proyectos_sea'].append(proyecto)
                    
        logger.info(f"üìä Formateado para informe: {len(resultado['proyectos_sea'])} proyectos SEA")
        
        return resultado

# Funci√≥n de test
if __name__ == "__main__":
    logger.info("=== TEST SCRAPER AMBIENTAL INTEGRADO ===")
    scraper = ScraperAmbiental()
    
    datos = scraper.obtener_datos_ambientales(dias_atras=3)
    
    print(f"\nüìä RESULTADOS:")
    print(f"   Proyectos SEA: {len(datos.get('proyectos_sea', []))}")
    
    if datos.get('metadata', {}).get('errores'):
        print(f"\n‚ö†Ô∏è Errores encontrados:")
        for error in datos['metadata']['errores']:
            print(f"   - {error}")