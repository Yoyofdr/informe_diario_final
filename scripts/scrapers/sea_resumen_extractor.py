#!/usr/bin/env python3
"""
Extractor de res√∫menes ejecutivos de proyectos SEA
Obtiene la descripci√≥n detallada de cada proyecto desde su ficha
"""

import logging
import requests
from bs4 import BeautifulSoup
import re
import time
from typing import Optional, Dict
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import json

logger = logging.getLogger(__name__)

class SEAResumenExtractor:
    def __init__(self):
        self.base_url = "https://seia.sea.gob.cl"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.8'
        })
    
    def extraer_resumen_proyecto(self, id_expediente: str) -> Dict[str, str]:
        """
        Extrae el resumen ejecutivo de un proyecto usando su ID
        """
        resultado = {
            'resumen': '',
            'inversion': '',
            'ubicacion': '',
            'titular': ''
        }
        
        try:
            # Intentar primero con requests directo
            resultado_requests = self._extraer_con_requests(id_expediente)
            if resultado_requests.get('resumen'):
                return resultado_requests
            
            # Si no funciona, intentar con Selenium
            logger.info(f"Intentando con Selenium para proyecto {id_expediente}")
            resultado_selenium = self._extraer_con_selenium(id_expediente)
            if resultado_selenium.get('resumen'):
                return resultado_selenium
                
        except Exception as e:
            logger.error(f"Error extrayendo resumen del proyecto {id_expediente}: {e}")
        
        return resultado
    
    def _extraer_con_requests(self, id_expediente: str) -> Dict[str, str]:
        """
        Intenta extraer informaci√≥n usando requests
        """
        resultado = {
            'resumen': '',
            'inversion': '',
            'ubicacion': '',
            'titular': ''
        }
        
        # URLs a probar - la URL del expediente funciona mejor
        urls = [
            f"{self.base_url}/expediente/expediente.php?id_expediente={id_expediente}",
            f"{self.base_url}/expediente/ficha/fichaPrincipal.php?modo=ficha&id_expediente={id_expediente}",
            f"{self.base_url}/expediente/expedientesEvaluacion.php?id_expediente={id_expediente}"
        ]
        
        for url in urls:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Eliminar scripts y estilos para obtener texto limpio
                for script in soup(['script', 'style']):
                    script.decompose()
                
                # Obtener texto completo
                texto = soup.get_text()
                lineas = [linea.strip() for linea in texto.splitlines() if linea.strip()]
                
                # Buscar informaci√≥n en el texto
                for i, linea in enumerate(lineas):
                    linea_lower = linea.lower()
                    
                    # Buscar descripci√≥n del proyecto
                    if 'descripci√≥n del proyecto' in linea_lower:
                        # Capturar las siguientes l√≠neas hasta encontrar otra secci√≥n
                        descripcion_lineas = []
                        for j in range(i+1, min(i+20, len(lineas))):
                            siguiente = lineas[j]
                            # Detener si encontramos otra secci√≥n
                            if any(seccion in siguiente.lower() for seccion in ['estado actual', 'titular', 'ubicaci√≥n', 'menu']):
                                break
                            descripcion_lineas.append(siguiente)
                        
                        if descripcion_lineas:
                            resultado['resumen'] = ' '.join(descripcion_lineas)[:2000]
                    
                    # Buscar monto de inversi√≥n
                    elif 'monto de inversi√≥n' in linea_lower or 'inversi√≥n' in linea_lower:
                        # La siguiente l√≠nea suele tener el monto
                        if i+1 < len(lineas):
                            monto = lineas[i+1]
                            if any(moneda in monto for moneda in ['USD', 'US$', 'D√≥lares', 'Millones']):
                                resultado['inversion'] = monto
                    
                    # Buscar ubicaci√≥n
                    elif 'comuna' in linea_lower or 'regi√≥n' in linea_lower:
                        if not resultado['ubicacion'] and i+1 < len(lineas):
                            ubicacion = lineas[i] + ' ' + lineas[i+1] if i+1 < len(lineas) else lineas[i]
                            if any(lugar in ubicacion for lugar in ['Atacama', 'Copiap√≥', 'Antofagasta', 'Santiago', 'Valpara√≠so']):
                                resultado['ubicacion'] = ubicacion[:200]
                    
                    # Buscar titular
                    elif 'titular' in linea_lower or 'proponente' in linea_lower:
                        if i+1 < len(lineas):
                            posible_titular = lineas[i+1]
                            if len(posible_titular) > 5 and not any(skip in posible_titular.lower() for skip in ['menu', 'buscar', 'inicio']):
                                resultado['titular'] = posible_titular[:200]
                
                if resultado['resumen']:
                    logger.info(f"‚úÖ Resumen encontrado con requests para {id_expediente}")
                    return resultado
                    
            except Exception as e:
                logger.debug(f"Error con URL {url}: {e}")
                continue
        
        return resultado
    
    def _extraer_con_selenium(self, id_expediente: str) -> Dict[str, str]:
        """
        Extrae informaci√≥n usando Selenium para p√°ginas con JavaScript
        """
        resultado = {
            'resumen': '',
            'inversion': '',
            'ubicacion': '',
            'titular': ''
        }
        
        driver = None
        try:
            # Configurar Chrome
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            
            # En Heroku usar el Chrome instalado
            import os
            if os.environ.get('DYNO'):
                chrome_options.binary_location = "/app/.chrome-for-testing/chrome-linux64/chrome"
                from selenium.webdriver.chrome.service import Service
                service = Service("/app/.chrome-for-testing/chromedriver-linux64/chromedriver")
                driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                from selenium.webdriver.chrome.service import Service
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # Navegar a la p√°gina
            url = f"{self.base_url}/expediente/ficha/fichaPrincipal.php?modo=ficha&id_expediente={id_expediente}"
            driver.get(url)
            
            # Esperar que cargue
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "table"))
            )
            
            # Buscar informaci√≥n en todas las tablas
            tablas = driver.find_elements(By.TAG_NAME, "table")
            
            for tabla in tablas:
                filas = tabla.find_elements(By.TAG_NAME, "tr")
                for fila in filas:
                    celdas = fila.find_elements(By.TAG_NAME, "td")
                    if len(celdas) >= 2:
                        header = celdas[0].text.strip().lower()
                        contenido = celdas[1].text.strip()
                        
                        # Mapear campos
                        if any(palabra in header for palabra in ['resumen', 'descripci√≥n', 'objeto']):
                            if len(contenido) > 100:
                                resultado['resumen'] = contenido[:2000]
                        elif any(palabra in header for palabra in ['inversi√≥n', 'monto', 'usd', 'millones']):
                            resultado['inversion'] = contenido
                        elif any(palabra in header for palabra in ['ubicaci√≥n', 'comuna', 'regi√≥n']):
                            resultado['ubicacion'] = contenido
                        elif any(palabra in header for palabra in ['titular', 'proponente', 'empresa']):
                            resultado['titular'] = contenido
            
            # Si no encontramos resumen en tablas, buscar en divs
            if not resultado['resumen']:
                divs_contenido = driver.find_elements(By.CLASS_NAME, "contenido") + \
                                driver.find_elements(By.CLASS_NAME, "descripcion") + \
                                driver.find_elements(By.CLASS_NAME, "resumen")
                
                for div in divs_contenido:
                    texto = div.text.strip()
                    if texto and len(texto) > 200 and len(texto) < 5000:
                        resultado['resumen'] = texto[:2000]
                        break
            
            if resultado['resumen']:
                logger.info(f"‚úÖ Resumen encontrado con Selenium para {id_expediente}")
            
        except Exception as e:
            logger.error(f"Error con Selenium: {e}")
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
        
        return resultado
    
    def obtener_id_de_url(self, url: str) -> Optional[str]:
        """
        Extrae el ID del expediente desde una URL del SEA
        """
        # Buscar patr√≥n id_expediente=XXXXX
        match = re.search(r'id_expediente=(\d+)', url)
        if match:
            return match.group(1)
        
        # Buscar otros patrones posibles
        match = re.search(r'/(\d{10,})', url)
        if match:
            return match.group(1)
        
        return None


# Instancia global
sea_resumen_extractor = SEAResumenExtractor()


if __name__ == "__main__":
    # Prueba con el proyecto Pimpollo
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*80)
    print("üîç PRUEBA DE EXTRACCI√ìN DE RESUMEN - PROYECTO PIMPOLLO")
    print("="*80)
    
    extractor = SEAResumenExtractor()
    
    # ID del proyecto Pimpollo
    id_pimpollo = "2166144419"
    
    print(f"\nExtrayendo informaci√≥n del proyecto ID: {id_pimpollo}")
    print("Esto puede tomar unos segundos...")
    resultado = extractor.extraer_resumen_proyecto(id_pimpollo)
    
    print("\nüìä RESULTADOS:")
    print("-"*40)
    
    if resultado['resumen']:
        print(f"‚úÖ RESUMEN: {resultado['resumen'][:500]}...")
        print(f"   Longitud: {len(resultado['resumen'])} caracteres")
    else:
        print("‚ùå No se encontr√≥ resumen")
    
    if resultado['inversion']:
        print(f"üí∞ INVERSI√ìN: {resultado['inversion']}")
    
    if resultado['ubicacion']:
        print(f"üìç UBICACI√ìN: {resultado['ubicacion']}")
    
    if resultado['titular']:
        print(f"üè¢ TITULAR: {resultado['titular']}")